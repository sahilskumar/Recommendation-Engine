{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Locality-sensitive-hashing(content-based).ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1nnSfjI5F1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from datasketch import MinHash, MinHashLSHForest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5OQdXz6WF9",
        "colab_type": "text"
      },
      "source": [
        "# DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_UFmfZm6aiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\n",
        "    tokens = text.lower()\n",
        "    tokens = tokens.split()\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06VaFFoe6eOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b78abec8-2d30-4771-ffdd-1c78d1501bbf"
      },
      "source": [
        "text = 'The devil went down to Georgia'\n",
        "print('The shingles (tokens) are:', preprocess(text))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shingles (tokens) are: ['the', 'devil', 'went', 'down', 'to', 'georgia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHvUqhlb6x4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CHOOSE YOUR PARAMETERS\n",
        "\n",
        "#Number of Permutations\n",
        "permutations = 128\n",
        "\n",
        "#Number of Recommendations to return\n",
        "num_recommendations = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWx1GRP_65B5",
        "colab_type": "text"
      },
      "source": [
        "# CREATING MINHASH FOREST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvi_qJgv69EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_forest(data, perms):\n",
        "  start_time = time.time()\n",
        "  \n",
        "  minhash = []\n",
        "  \n",
        "  for text in data['text']:\n",
        "    tokens = preprocess(text)\n",
        "    m = MinHash(num_perm = perms) #setting the permutation in minhash\n",
        "    for s in tokens:\n",
        "      m.update(s.encode('utf-8'))\n",
        "    minhash.append(m)\n",
        "    \n",
        "  forest = MinHashLSHForest(num_perm = perms)\n",
        "  \n",
        "  for i,m in enumerate(minhash):\n",
        "        forest.add(i,m)\n",
        "     \n",
        "  forest.index()\n",
        "  print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
        "    \n",
        "  return forest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwoqwxzY9Mea",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1AsaxIB9Kn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(text, database, perms, num_results, forest):\n",
        "  start_time = time.time()\n",
        "  \n",
        "  tokens = preprocess(text)\n",
        "  m = MinHash(num_perm=perms)\n",
        "  for s in tokens:\n",
        "    m.update(s.encode('utf8'))\n",
        "    \n",
        "  idx_array = np.array(forest.query(m, num_results))\n",
        "  \n",
        "  if len(idx_array) == 0:\n",
        "        return None # if your query is empty, return none\n",
        "    \n",
        "  result = database.iloc[idx_array]['title']\n",
        "  \n",
        "  print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
        "    \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Foiw2IEhBt13",
        "colab_type": "text"
      },
      "source": [
        "# Testing Our Recommendation Engine on NIPS Conference Papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex391fwfBxKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d benhamner/nips-papers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVlRVhWFC1DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip nips-papers.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBd11swODArA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f1e7487b-59d0-4721-fd5e-6e5cea4a7634"
      },
      "source": [
        "df = pd.read_csv(\"papers.csv\")\n",
        "df['text'] = df['title']+\" \"+df['abstract']\n",
        "forest = create_forest(df, permutations)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It took 13.716139793395996 seconds to build forest.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qy2_OfiDuUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "b71c983e-9a42-4dc2-864c-16cff3192e68"
      },
      "source": [
        "num_recommendations = 5\n",
        "title = 'Artificial Network'\n",
        "result = predict(title, df, permutations, num_recommendations, forest)\n",
        "print('\\n Top Recommendation(s) is(are) \\n', result)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It took 0.007211923599243164 seconds to query forest.\n",
            "\n",
            " Top Recommendation(s) is(are) \n",
            " 5443    A Generalization of Submodular Cover via the D...\n",
            "4485                                   Compete to Compute\n",
            "6794        Do Deep Neural Networks Suffer from Crowding?\n",
            "5365    Inferring Algorithmic Patterns with Stack-Augm...\n",
            "5918    Theoretical Comparisons of Positive-Unlabeled ...\n",
            "Name: title, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}